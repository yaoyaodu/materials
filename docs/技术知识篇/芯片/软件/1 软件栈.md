# 软件栈

核心概念：

* 框架Framework
* 抽象体系：**AI软件栈**的本质是基于某款AI硬件，用于**衔接上层AI应用的抽象体系**。
* **软件抽象**：因为不可能把**底层硬件**直接暴露给**上层具体的AI应用**，那样**太影响生产效率**，所以引入了**一层又一层的AI软件抽象**，构成了AI软件栈。
* 在这个软件栈层次图里，**每个building block其实都试图提供一种抽象**。
* CUDA核心：**CUDA核心数量决定了GPU并行处理的能力**，在深度学习、机器学习等**并行计算**类业务下，CUDA核心多意味着性能好一些。



曦云C500系列通用GPU采用沐曦自主研发架构，提供强大的多精度混合算力，同时配以大容量存储和新一代高速IO接口及多卡互联技术。C500系列产品可广泛应用于**人工智能、数据中心、科学计算以及教育和科研**等场景。

应用场景：Application Scenarios

* AI推理/训练：AI Inference/Training
* 科学计算：Scientific Computing
* 数据分析：Data Analysis
* 数据中心：Data Center



**软件栈：Software Stack**

* **Application Layer**: 
  * **AI Frameworks（AI框架）**: PyTorch, TensorFlow, Caffe, Paddle, OneFlow
  * **AI and Science Ecosystem**: 
    * 智能交通：Smart Transportation
    * 医疗健康：Healthcare
    * 工业4.0：Industry 4.0
    * 安防：Security
    * 推荐系统：Recommendation System
    * 游戏/视频：Gaming/Video
  * **Science**: 
    * 分子动力学：Molecular Dynamics
    * 空气动力学：Aerodynamics
    * 流体力学：Fluid Mechanics
    * 安全应用：Security Application
    * 地质、石油：Geology ( [dʒiˈɑlədʒi]), Petroleum
    * 金融：Finance
    * 气象：Meteorology
    * 海洋气候：Marine Climate
    * 大数据：Big Data

* MACAMACA Libs: 
  * **MACA DNN**
  * ONNX-RT/MACA-RT
  * Converter, Quantizer
  * **BLAS, FFT, Sparse,** Solver, Random, Eigen, Thrust, CUB
  * MCCL, MPI
* MACAMACA  Language:
  * MACA C++, Fortran, OpenMP, OpenACC, Python, OpenCL
* MACAMACA Drivers: 
  * KMD 
  * UMD 
  * RDMA
  * **Virtualization**
  * **Docker/Singularity**
  * **Profiler**
  * **SMI**
* Hardware
  * MetaX C500 

说明：

**mxcc** 

* Overview: 
  * mxcc is the **compiler** for MetaX GPUs. It can compile MACAMACA souce files into an executable file that has the host part and the device part. Each part runs on the corresponding side of the environment.

* Usage:
* Preparation Before Using: The **full release package of MACAMACA**, which includes **MACAMACA runtime and mxcc toolchain**, must be installed in advance. 
* Supported Options
* Examples: 操作类型和示例代码

**mxobjdump** 

- Overview: mxobjdump is a tool that can extract device binary file or **display device side information** directly from a host executable file. 

- Usage: 
- Supported Options
- Examples: 操作类型和示例代码

说明：objdump 有点像那个快速查看之类的工具，就是**以一种可阅读的格式让你更多地了解二进制文件可能带有的附加信息。**





**PyTorch (tensor operations)**

* Installing PyTorch-MACA

  * PyTorch-MACA is based on PyTorch v1.10.0-rc3. 

  * It works in a MACA environment and supports **tensor operations** both on CPU and MACA device.

* API: 

  * PyTorch **Operator API**
  * PyTorch CUDA/MACA **Runtime API**

* Samples （操作步骤示例？）：

**mcDNN （大数据、AI训练）**

* Introduction: 

  * mcDNN is a **Deep Neural Network library** offered by MetaX. 

  * Its **context-based API** allows for easy **multithreading** and (optional) **interoperability** with MACA **streams**.


* Example（操作步骤示例？）:
  * Build and Run
    * Set environment variable
    * Build
    * Run

  * Source Code
    * Create handle.
    * Create tensor and filter descriptors and set paramaters for them.
    * Allocate device memory for input, filter, output, and workspace, and copy host data to device.
    * Copy host data from host to device. 
    * Call convolution forward function. 
    * Copy output data back to host memory.
    * Release any resources such as device memory, descriptors, and handler. 

* mcDNN API
  * Handle
    * **Data Type Reference**
      * mcdnnStatus_t
        * mcdnnStatus_t is an enumerated type used for function status returns. All mcDNN library functions return their status, which can be one of the following values:
          * 值和相应的描述

      * mcdnnHandle_t
        * mcdnnHandle_t is a pointer to an opaque structure holding the mcDNN library context. 

    * **API Functions**
      * mcdnnCreate(): 
        * 代码示例
        * 描述：This function initializes the mcDNN library and creates a handle to an opaque structure holding the mcDNN library context. It allocates hardware resources on the host and device and must be called prior to making any other mcDNN library calls.
        * Parameters: handle 参数描述
        * Returns: 返回值和相应描述

  * Tensor
    * **Data Type Reference**
    * **API Functions**

  * Convolutional Layer


**mcFFT (科学计算；数学库)**

* mcFFT: MACA **Fast Fourier Transform** library （科学计算）
* The mcFFT library provides **a simple interface** for **computing FFTs** on an MetaX GPU, which allows users to quickly leverage the **floating-point power** and **parallelism** of the GPU in a highly optimized and tested FFT library.
* **mcFFT is a library** released along with the **MACA toolkit**. For the installation of the MACA toolkit, refer to MACA Quick Start Guide.

**mcBLAS （科学计算；数学库）**

* The mcBLAS library is an **implementation of BLAS** (**Basic Linear Algebra Subprograms**) on top of the MetaX MACA **runtime**. It allows the user to **access the computational resources** of MetaX GPU. 

* To use the **mcBLAS API**, the application must allocate the required **matrices and vectors** in the **GPU memory** space, fill them with data, call the sequence of desired mcBLAS functions, and then **upload the results from the GPU memory space back to the host**. The mcBLAS API also provides helper functions for **writing and retrieving data from the GPU**.
  
* **The aim of mcBLAS is to provide:**
- **functionality similar to Legacy BLAS, adapted to run on GPUs**


  - **high performance robust implementation**


* **mcBLAS** is written in C++14 and MACA. **It uses MetaX’s MACA runtime to run on GPU devices**.

* BLAS(**基础线性代数程序集**)是进行**向量和矩阵等基本线性代数操作**的事实上的**数值库**。**这些程序**最早在1979年发布，是LAPACK(Linear Algebra PACKage)的一部分，便于**建立功能更强的数值程序包**。BLAS库在**高性能计算中被广泛应用**，由此衍生出大量优化版本，如Intel 的Intel MKL，**AMD的ACML**，Goto BLAS和ATLAS等非硬件厂商优化版本，以及**利用GPU计算技术实现的CUBLAS**等。



**mcSPARSE (科学计算；数学库)**

* The API Reference guide for **mcSPARSE**, the **MACA Sparse Basic Linear Algebra Subroutine** library. (sparse表示**稀疏矩阵**)
* The mcSPARSE library is an implementation of basic linear algebra subroutines for **sparse matrices and vectors** on top of the MetaX **MACA runtime (which is part of MACA Toolkit)**. The library is designed to be called from C++ code. **It uses MetaX's MACA runtime to run on GPU devices.** The mcSPARSE targets **matrices with a number of (structural) zero elements** which represent about 95% of the total entries. 
* To use the **mcSPARSE API**, the application must allocate the required **matrices and vectors** in the **GPU memory** space, fill them with data, call the sequence of desired mcSAPRSE functions, and then **upload the results from the GPU memory space back to the host**. The mcBLAS API also provides helper functions for **writing and retrieving data from the GPU**.
* The **functionality** of mcSPARSE is organized in the following categories: 
  * **Sparse Helper Functions** describe available helper functions that required for **subsequent library calls**. 
  * **Sparse BLAS Level 1 Functions** describe operations between **a sparse vector and a dense vector**. 
  * **Sparse BLAS Level 2 Functions** describe operations between **a sparse matrix and a dense vector**. 
  * **Sparse BLAS Level 3 Functions** describe operations between **a sparse matrix and a sparse or dense matrix**. 
  * **Sparse Conversion Function** describe **operations on a matrix in sparse format** to obtain a different matrix format.



**mcSOLVER （科学计算；数学库）**



**mcRAND （科学计算；数学库）**



**mcTHRUST （科学计算；数学库）**



**MACAMACA Quick Start Guide**

* Overview:
  * GPGPU
  * MACA: A General Purpose Parallel Computing Platform and Programming Model, which aims for applications with a high degree of parallelism deployed on MetaX GPUs. 
  * 
  * 

* MACA Installation
* Programming Model
* Programming with MACA
* Transiting from CUDA to MACA
* Appendixes





## AMD ROCm

AMD ROCm is the first open-source software development platform for HPC/Hyperscale-class GPU computing. AMD ROCm brings the UNIX philosophy of choice, minimalism and modular software development to GPU computing.

Since the ROCm ecosystem is comprised of **open technologies**: **frameworks (Tensorflow / PyTorch), libraries (MIOpen / Blas / RCCL), programming model (HIP), inter-connect (OCD) and up streamed Linux® Kernel support** – the platform is continually optimized for performance and extensibility. Tools, guidance and insights are shared freely across the **ROCm GitHub community and forums**.

Note: The AMD ROCm™ open software platform is a compute stack for headless system deployments. GUI-based software applications are currently not supported.

**Complete exascale solution for ML/HPC:**

- Applications: HPC apps, ML frameworks
- **Cluster deployment: Singularity, SLURM, Docker, Kubernates**
- Tools: Debugger, Profiler, Tracer, System Valid. System Mgmt.
- Portability Frameworks: Kokkos, Magma, GridTools, ONNX
- **Math libraries**: RNC, FFT, Sparse, BLAS, Eigen, MIOpen
- Scale-out **communication libraries**: OpenMPI, UCX, MPICH, RCCL
- **Programmming Models**: OpenMP, HIP, OpenCL
- Processors: CPU + GPU

![image-20220816091259865](C:\Users\杜瑶瑶\AppData\Roaming\Typora\typora-user-images\image-20220816091259865.png)





**AI软件栈**的本质是基于某款AI硬件，用于**衔接上层AI应用的抽象体系**。

因为我们不可能把底层硬件直接暴露给上层具体的AI应用，那样太影响生产效率，所以引入了一层又一层的AI软件抽象，构成了AI软件栈。以NV的软件栈为例。

- 为了让上层应用在**不需要touch到SASS 汇编的情况下享受到开发NV平台高性能程序的福利**，有了**CUDA**。
- 为了让**上层应用**在**不需要关注底层CUDA优化细节**的情况下享受到**高性能GEMM库**的红利，有了**cuBLAS**。
- 为了让**AI specific的计算需求**在**不需要关注底层实现细节**的情况下有效发挥NV GPU的性能，有了**cuDNN**(对**convolution，attention，BN**等**常见AI算子的支持**)。
- 为了让**AI布署**需求不需要关注太多底层实现细节即可有效发挥NV GPU的性能，于是有了**TensorRT**。更进一步，为了**简化TensorRT集成在推理布署服务系统里的overhead**，诞生了**Triton Inference Server**。
- 为了让**AI训练**需求在不需要关注**底层硬件拓扑互联**细节的情况下有效发挥**互联性能**，于是有了**NCCL**。先是1.0的单机多卡，然后是2.0的单机多卡支持。
- 为了简化NV GPU上**超大规模语言模型类的训练负担**，于是有了Megatron-LM。
- 为了**给上层AI应用提供更大的灵活性来根据上层需求来定制计算优化策略**，于是有了**CUTLASS**(cuDNN V8也有一些性质相似的尝试，比如fusion)。
- 为了让AI业务层获得**开箱即用的AI框架**使用体验，于是有了NGC里的TensorFlow/PyTorch等一众AI框架的NV优化加持。

可以把NV的软件栈抽象层次做一个梳理展示如下：

* Megatron-LM, Triton

* NV TensorFlow, PyTorch, TensorRT
* NCCL, CUTLASS, cuDNN, cuBLAS
* CUDA
* SASS
* NV GPU

在这个软件栈层次图里，**每个building block其实都试图提供一种抽象**。比如：

- Megatron-LM想提供的是**针对大模型训练的抽象**。 NV TensorFlow和NV PyTorch是**AI框架的抽象**。
- Triton是**服务布署系统的抽象**。TensorRT是**推理优化工具的抽象**。
- 这些抽象模块之间又存在分层。比如**Megatron-LM依赖于NV-PyTorch的AI框架优化能力**。**Triton会用到TensorRT暴露的调用接口**。

同时，这里面几乎所有的抽象都存在**leaky abstraction**的问题(对应于图片里的红线)，在Raja Koduri 在Hotchips20的[《No Transistor Left Behind》](https://link.zhihu.com/?target=https%3A//www.intel.com/content/www/us/en/newsroom/news/raja-koduri-keynote-hot-chips-2020.html)的keynote里有一页形象的slides也emphasize了这个问题(图片里的洞表示了leaky abstraction的存在)。

[再谈AI软件栈 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/393041071)

![image-20220815113707619](C:\Users\杜瑶瑶\AppData\Roaming\Typora\typora-user-images\image-20220815113707619.png)