# 软件栈

核心概念：

* 框架Framework
* 抽象体系：**AI软件栈**的本质是基于某款AI硬件，用于**衔接上层AI应用的抽象体系**。
* **软件抽象**：因为不可能把**底层硬件**直接暴露给**上层具体的AI应用**，那样**太影响生产效率**，所以引入了**一层又一层的AI软件抽象**，构成了AI软件栈。
* 在这个软件栈层次图里，**每个building block其实都试图提供一种抽象**。
* CUDA核心：**CUDA核心数量决定了GPU并行处理的能力**，在深度学习、机器学习等**并行计算**类业务下，CUDA核心多意味着性能好一些。


## AMD ROCm

AMD ROCm is the first open-source software development platform for HPC/Hyperscale-class GPU computing. AMD ROCm brings the UNIX philosophy of choice, minimalism and modular software development to GPU computing.

Since the ROCm ecosystem is comprised of **open technologies**: **frameworks (Tensorflow / PyTorch), libraries (MIOpen / Blas / RCCL), programming model (HIP), inter-connect (OCD) and up streamed Linux® Kernel support** – the platform is continually optimized for performance and extensibility. Tools, guidance and insights are shared freely across the **ROCm GitHub community and forums**.

Note: The AMD ROCm™ open software platform is a compute stack for headless system deployments. GUI-based software applications are currently not supported.

**Complete exascale solution for ML/HPC:**

- Applications: HPC apps, ML frameworks
- **Cluster deployment: Singularity, SLURM, Docker, Kubernates**
- Tools: Debugger, Profiler, Tracer, System Valid. System Mgmt.
- Portability Frameworks: Kokkos, Magma, GridTools, ONNX
- **Math libraries**: RNC, FFT, Sparse, BLAS, Eigen, MIOpen
- Scale-out **communication libraries**: OpenMPI, UCX, MPICH, RCCL
- **Programmming Models**: OpenMP, HIP, OpenCL
- Processors: CPU + GPU

![image-20220816091259865](C:\Users\杜瑶瑶\AppData\Roaming\Typora\typora-user-images\image-20220816091259865.png)





**AI软件栈**的本质是基于某款AI硬件，用于**衔接上层AI应用的抽象体系**。

因为我们不可能把底层硬件直接暴露给上层具体的AI应用，那样太影响生产效率，所以引入了一层又一层的AI软件抽象，构成了AI软件栈。以NV的软件栈为例。

- 为了让上层应用在**不需要touch到SASS 汇编的情况下享受到开发NV平台高性能程序的福利**，有了**CUDA**。
- 为了让**上层应用**在**不需要关注底层CUDA优化细节**的情况下享受到**高性能GEMM库**的红利，有了**cuBLAS**。
- 为了让**AI specific的计算需求**在**不需要关注底层实现细节**的情况下有效发挥NV GPU的性能，有了**cuDNN**(对**convolution，attention，BN**等**常见AI算子的支持**)。
- 为了让**AI布署**需求不需要关注太多底层实现细节即可有效发挥NV GPU的性能，于是有了**TensorRT**。更进一步，为了**简化TensorRT集成在推理布署服务系统里的overhead**，诞生了**Triton Inference Server**。
- 为了让**AI训练**需求在不需要关注**底层硬件拓扑互联**细节的情况下有效发挥**互联性能**，于是有了**NCCL**。先是1.0的单机多卡，然后是2.0的单机多卡支持。
- 为了简化NV GPU上**超大规模语言模型类的训练负担**，于是有了Megatron-LM。
- 为了**给上层AI应用提供更大的灵活性来根据上层需求来定制计算优化策略**，于是有了**CUTLASS**(cuDNN V8也有一些性质相似的尝试，比如fusion)。
- 为了让AI业务层获得**开箱即用的AI框架**使用体验，于是有了NGC里的TensorFlow/PyTorch等一众AI框架的NV优化加持。

可以把NV的软件栈抽象层次做一个梳理展示如下：

* Megatron-LM, Triton

* NV TensorFlow, PyTorch, TensorRT
* NCCL, CUTLASS, cuDNN, cuBLAS
* CUDA
* SASS
* NV GPU

在这个软件栈层次图里，**每个building block其实都试图提供一种抽象**。比如：

- Megatron-LM想提供的是**针对大模型训练的抽象**。 NV TensorFlow和NV PyTorch是**AI框架的抽象**。
- Triton是**服务布署系统的抽象**。TensorRT是**推理优化工具的抽象**。
- 这些抽象模块之间又存在分层。比如**Megatron-LM依赖于NV-PyTorch的AI框架优化能力**。**Triton会用到TensorRT暴露的调用接口**。

同时，这里面几乎所有的抽象都存在**leaky abstraction**的问题(对应于图片里的红线)，在Raja Koduri 在Hotchips20的[《No Transistor Left Behind》](https://link.zhihu.com/?target=https%3A//www.intel.com/content/www/us/en/newsroom/news/raja-koduri-keynote-hot-chips-2020.html)的keynote里有一页形象的slides也emphasize了这个问题(图片里的洞表示了leaky abstraction的存在)。

[再谈AI软件栈 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/393041071)

![image-20220815113707619](C:\Users\杜瑶瑶\AppData\Roaming\Typora\typora-user-images\image-20220815113707619.png)